{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Fundamentals\n",
    "------\n",
    "\n",
    "<br>\n",
    "\n",
    "<center><img src=\"http://3.bp.blogspot.com/-7RWgohC4pYE/VhtQ8IELsLI/AAAAAAAAA6I/_XFhMbjpcCY/s1600/Simple%2BNeural%2BNetwork.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### By The End Of This Session You Should Be Able To:\n",
    "\n",
    "*  Describe the origins of Neural Networks\n",
    "*  Diagram the fundamental building block of DL: The Perceptron\n",
    "*  Define the nouns and verbs of NN\n",
    "*  Remember the sigmoid function as the goto nonlinear function\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Origins of Neural Nets\n",
    "\n",
    "Neural nets derive their inspiration from animal nervous systems.  The wikipedia page on [neurons](https://en.wikipedia.org/wiki/Neuron) gives a good illustration of the wiring of an animal nervous system. \n",
    "\n",
    "Mccolloch and Pitts (MCP) developed an abstract version of a neuron and proposed analyzing networks of these abstract neurons as a way to understand and simulate the operation of their biological equivalents. \n",
    "\n",
    "Part of the hope (and hype) was that networks of these abstract \"neurons\" would give insight into the operation of animal brains and perhaps offer some of the same functionality (recall Marvin the computer in \"Hitchhiker's Guide to the Galaxy\" - \"I'm so depressed\").  The wiki page on [artificial neural networks](https://en.wikipedia.org/wiki/Artificial_neuron) gives a good summary of the abstract neurons stemming from the MCP neuron.  \n",
    "\n",
    " \n",
    "The most general arrangement of neuron connections is the \"fully connected\" network wherein every neuron is an input to every other neuron.  Fully-connected neural nets are not (yet) practical in applications, for a number of reasons.\n",
    "\n",
    "The neural networks used in applications have more limited structures.  The wiki page on [artificial neural networks](https://en.wikipedia.org/wiki/Artificial_neural_network) shows the structure for one of the most common architectures - the feed-forward network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For Review:\n",
    "\n",
    "* What does it mean to say a neuron fires?\n",
    "* What causes a neuron to fire?\n",
    "* What are the inputs to a neuron and how many of them are there (fan-in)?\n",
    "* What are the outputs of a neuron and how many of them are there (fan-out)?\n",
    "* What elements of a real neuron are captured by the MCP neuron? What elements are not?\n",
    "* How would you connect MCP neurons into a network? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Perceptron\n",
    "\n",
    "The perceptron is the simplest kind of neural net unit. It is effectively a modern representation of the [MCP neuron](http://www.mind.ilstu.edu/curriculum/modOverview.php?modGUI=212). It is not necessarily the strongest kind of unit for all applications, but it is the most basic and general, and best for learning on and overcoming the basic concepts of neural nets. \n",
    "\n",
    "Perceptron\n",
    "-----\n",
    "\n",
    "<center><img src=\"images/perceptron.png\" width=\"500\" /></center>\n",
    "\n",
    "The simple perceptron has 4 basic components (it is **not** limited in any particular case to only 3 inputs - one case we will review has only two inputs):\n",
    "\n",
    "1. Input\n",
    "1. Weights\n",
    "1. Bias \n",
    "1. Output\n",
    "\n",
    "\n",
    "In the case of the perceptron, we need only sum the product of the weights and inputs and add them to the bias. \n",
    "\n",
    "$$ y_{j} = \\sum\\limits_{k=1}^n w_{kj}x_{j} + b_{j} $$\n",
    "\n",
    "For an individual simple perceptron $j$ and weights $k$ We can write this in vector notation as follows:\n",
    "\n",
    "$$ Y = Wâ‹…X+b$$\n",
    "\n",
    "This is, in effect, a kind of weird linear regression! More on perceptrons soon. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Artifical Neurons in General\n",
    "\n",
    "As we expand on the idea of an ANN, we can generalize the above structure to the more general format and notation:\n",
    "\n",
    "<center><img src=\"images/network.png\" height=\"500\"/></center>\n",
    "\n",
    "Each node has 2 functions (methods in CS talk):\n",
    "\n",
    "1. A linear function that calculates the weighted sum\n",
    "\n",
    "2. A non-linear function that applies the non-linear function to the output from the linear node\n",
    "\n",
    "For any basic artifical neuron $j$ within a network of neurons, we can model the **output** $o_{j}$ with the following prinicpal equation:\n",
    "\n",
    "$$o_{j} = \\psi( \\sum\\limits_{k=1}^m w_{kj}x_{j} + \\theta_{j}) $$\n",
    "\n",
    "We differentiate the **output** $o$ of a neuron from a predicted variable $y$ because it is the combination of outputs that allows the network to make predictions.\n",
    "\n",
    "### The activation function\n",
    "\n",
    "The above function $\\psi$ is pretty important, because it determines how the ANN is going to respond to a set of stimuli. Remember, real neurons can fire (give som positive response) or not fire (no response). There is no other way. So the shape of this function is particularly important, and is one of the most important topics in the field.\n",
    "\n",
    "Let us recall the football game exercise we went through at the end of the last session. A team could either win or not win, there was no other way, and we found a function that would fit that goal nicely: the logistic function.\n",
    "\n",
    "$$ f(z) = \\frac{e^{z}}{1+e^{z}} = \\frac{1}{1+e^{-z}}$$\n",
    "\n",
    "Unsurprisingly, the most common activation function employed in ANNs is a logistic function. It's not the only one, it's just the most general one. Let's explore it's use now. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def sigmoid(z): # couldn't hurt to think functionally here, for those in the know\n",
    "    \"\"\"\n",
    "    Define sigmoid activation function to scalar or vector\n",
    "    Input: z (int, float, np.array)\n",
    "    Output: sigmoid (float, np.array)\n",
    "    \"\"\"\n",
    "\n",
    "    return 1./(1+np.exp(-z))\n",
    "\n",
    "domain = np.arange(-6, 6, 0.01)\n",
    "plt.plot(domain, sigmoid(domain), linewidth=2);\n",
    "plt.grid(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why is this beneficial? Well it provides a binary output (0 or 1) and it provides an estimate of probability via likelihoods (review previous lecture if confused). \n",
    "\n",
    "However there are many types of activation functions, all have been proven necessary at one point or another in order to capture nonlinearities, which are an essential component of working neural nets (NN), as they are necessary for the net to learn complex decision boundaries:\n",
    "\n",
    "Activation Functions\n",
    "----\n",
    "\n",
    "<center><img src=\"images/activation_functions.png\" height=\"400\" width=\"850\"/></center>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep learning and Layers\n",
    "\n",
    "Perceptrons are limited in the number of tasks they can do alone. Just like a neural network in animals, in order to do something interesting, they need to have lots of neighbors and include the use of layers, just as animals do. Most useful NN include 1 or more hidden layers:\n",
    "\n",
    "<center><img src=\"images/simple.png\" height=\"500\"/></center>\n",
    "\n",
    "What does the hidden layer do? Just as in animals, it learns abstractions of the input data, essentially automatically engineering its own features in order to learn its task. \n",
    "\n",
    "** Note: **\n",
    "In human brains, neurons are organized exactly this way, with a single abstract idea at the bottom of a \"tower\" consisting of several layers of abstraction. At the top level of the tower is raw input data from senses. Each layer (it is believed there are a handful) down refines this data and provides additional information so as to form a concrete abstraction composed of examples such as \"shoe\". These data are cross associated with other parts of the brain to perform retrieval of rich and complex ideas. Memories are stored in similar ways, being associated with both higher and lower level parts of the tower. This is why when you think \"shoe\" it brings back a set of queues that access different parts of your memory. (For example, I remember my shiny brown shoes as a little boy and my mom helping me tie them, the day I bought my favorite boots and many other memories of the like.)\n",
    "\n",
    "\n",
    "So we can think of a basic NN as a system of chained logistic regressions, each dictating the input of the other.\n",
    "\n",
    "<center><img src=\"http://1.bp.blogspot.com/-ZCOvJ9OYHLE/T8jXLkG81XI/AAAAAAAAAto/7LTHWLqOMyg/s1600/p1.png\" height=\"500\"/></center>\n",
    "\n",
    "Having several layers to your NN makes it \"deep\" learning. \n",
    "\n",
    "\n",
    "<center><img src=\"images/complex.png\" height=\"500\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
