{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build your first NN from scratch\n",
    "The code block below generates data for a regression problem.  The generator is similar to the generator for the classification problem that you've been working of for a while.  The main difference is how the labels are related to the attributes.  These labels are going require you to change several things.  For one thing, you may not want to use the sigmoid non-linearity.  Why?  For another, you'll have to define a loss function and work out the gradient equations.  \n",
    "\n",
    "Follow the same steps that you've seen a couple of times now.  \n",
    "1.  Define the inference model - network architecture\n",
    "2.  Define the loss function\n",
    "3.  Derive the gradient wrt to weights\n",
    "4.  Use gradient descent to determine best parameters for your model.  \n",
    "Hint: You won't use a sigmoid function. Instead, you'll just use $\\hat{y} = w1 * x1 + w2 * x2$ instead. In the problem above, $\\hat{y} = sigmoid(w1 * x1 + w2 * x2)$. Hence, the gradient will be different from the problem above. Loss function is the same: Mean Squared Error.\n",
    "\n",
    "This is just the beginning.  Soon you'll be writing and solving much more complicated models than this - models having millions of free parameters instead of two.  But the principals and steps that you'll take to build and train those models will be exactly the same as for this simple one.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "#numpy version of Data Generator \n",
    "sd = 0.2\n",
    "nRows = 400\n",
    "X = np.random.uniform(0.0, 1.0, (nRows, 2))\n",
    "noise = np.random.normal(0.0, sd, (nRows,))\n",
    "idx = X[:, 1] > X[:, 0] + noise\n",
    "Y = np.zeros((nRows,))\n",
    "Y[idx] = 1.0\n",
    "\n",
    "#plot data\n",
    "colorMap = {0.0: \"blue\", 1.0: \"red\"}\n",
    "colors = [colorMap[c] for c in Y]\n",
    "plt.scatter(X[:,0], X[:, 1], c=colors)\n",
    "plt.xlabel('x1')\n",
    "plt.ylabel('x2')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Write a simple neural net using numpy\n",
    "The simple neural net that you saw before was easy enough to write out each of the attributes and weights as separate variables, but the nets you'll be training later will have millions of parameters.  To deal with those, you'll use matrices and vectors to represent the input data and the weights that will be applies.  Matrix notation facilitates writing functions that will process all of the data in a single pass. \n",
    "\n",
    "Below is written some pretty awful script-type code. I challenge you to vectorize the algorithm in production-grade python, using the neural_network.py class we wrote in lecture. Or, even better, write your own class. Make the bridge between what I have for you here and what is in class to vectorize your code and make it efficient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def npS(XX):\n",
    "    return 1.0 / (1.0 + np.exp(-XX))\n",
    "\n",
    "def npNeuralNet(XX, W):\n",
    "    return npS(XX.dot(W))\n",
    "\n",
    "def npLoss(XX, Y, W):\n",
    "    return ((Y - npNeuralNet(XX, W)) ** 2).mean()\n",
    "\n",
    "def npGradW(XX, Y, W):\n",
    "    arg = XX.dot(W)\n",
    "    terms = (-2.0 * (Y - npS(arg)) * npS(arg) * npS(-arg)).reshape(-1, 1)\n",
    "    return (terms * XX).mean(axis=0).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a way of organizing your training pattern for gradient descent using a technique called [minibatching](https://machinelearningmastery.com/gentle-introduction-mini-batch-gradient-descent-configure-batch-size/). I recommend taking apart the code line by line, running it and seeing what it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "W = np.zeros((2,))\n",
    "lr = 0.1\n",
    "miniBatchSize = 50\n",
    "nPasses = 300\n",
    "loss = []\n",
    "w_list = W\n",
    "\n",
    "startEnd = zip(range(0, len(X), miniBatchSize), range(miniBatchSize, len(X) + 1, miniBatchSize))\n",
    "\n",
    "for iPass in range(nPasses):\n",
    "    for (s, e) in startEnd:\n",
    "        x = X[s:e, ]\n",
    "        y = Y[s:e]\n",
    "        W = W - lr * npGradW(x, y, W)\n",
    "        if not iPass % 5:\n",
    "            loss.append(npLoss(x, y, W))\n",
    "            print(iPass, npLoss(x, y, W), W)\n",
    "            w_list = np.vstack((w_list, W))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "plt.plot(loss)\n",
    "plt.xlabel('Iterations')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Loss over Time\")\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "plt.xlabel(\"W1\")\n",
    "plt.ylabel(\"W2\")\n",
    "plt.title(\"Weights Updating Over Iterations\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(w_list)\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.ylabel(\"Weights: W1 and W2\")\n",
    "plt.title(\"Weights Updating Over Iterations\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "1.  Twiddle parameters (batch size and learning rate) and see what settings give you the fastest convergence. What happens to the graphs when you change the parameters?  \n",
    "2.  Build an iterator (class) that does the following\n",
    "    -when you instantiate a member of the class, it generates the data set and initializes a counter\n",
    "    -when you execute the member function \"data(mbSize)\" it generates the next mbSize points from the data set\n",
    "    -when the \"data\" function reaches the end of the data, it starts over. \n",
    "3.  Modify the training code above by removing the \"zip\" and replacing the double loop over the data with a single loop that hits the iterator for as many steps as you choose. \n",
    "4.  Modify the data generator class so that when the \"data\" function runs out of data, it randomly scrambles the data rows before starting through it again.  Explain why do this.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
